const express = require("express");
const cors = require("cors");
const multer = require("multer");
const dotenv = require("dotenv");
const { GoogleGenerativeAI } = require("@google/generative-ai");

dotenv.config();

const app = express();
const upload = multer();

app.use(cors());
app.use(express.json());

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const SYSTEM_PROMPT = `
ä½ æ˜¯ä¸€å€‹ AI åŠ©ç†...
`;

app.post("/api/chat", upload.single("image"), async (req, res) => {
  try {
    const userMessage = req.body.message || "";
    const imageFile = req.file;

    if (!userMessage && !imageFile) {
      return res.status(400).json({ error: "ç¼ºå°‘è¨Šæ¯æˆ–åœ–ç‰‡" });
    }

    const parts = [
      { text: SYSTEM_PROMPT },
      { text: userMessage }
    ];

    if (imageFile) {
      parts.push({
        inlineData: {
          data: imageFile.buffer.toString("base64"),
          mimeType: imageFile.mimetype,
        },
      });
    }

    const result = await model.generateContent({
      contents: [{ role: "user", parts }],
    });

    const replyText = result.response.text();
    res.json({ reply: replyText });

  } catch (err) {
    console.error(err);
    res.status(500).json({ error: "LLM å‘¼å«å¤±æ•—" });
  }
});  // â† é€™è£¡è¦æœ‰ï¼ä½ å¯èƒ½å°‘äº†é€™å€‹

// â­ï¸ ä½ çš„ç¨‹å¼å¯èƒ½å°‘äº†é€™ä¸€è¡Œçµå°¾ ðŸ‘‡
const PORT = process.env.PORT || 8080;
app.listen(PORT, () => {
  console.log(`AI Assistant backend listening on port ${PORT}`);
});
